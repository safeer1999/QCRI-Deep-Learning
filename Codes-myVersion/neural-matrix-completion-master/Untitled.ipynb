{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/melshrif/Documents/Matrix Completion Project/neural-matrix-completion-master'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.sparse\n",
    "import random\n",
    "import sys\n",
    "import time \n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, data_dir):\n",
    "        ''' data_dir: dataset directory\n",
    "            N: number of rows\n",
    "            M: number of columns\n",
    "        '''\n",
    "        self.data_dir = data_dir\n",
    "        self.rating_fname = data_dir + 'rating.npz'\n",
    "        self.tr_m_fname = data_dir + 'train_mask.npz'\n",
    "        self.v_m_fname = data_dir + 'val_mask.npz'\n",
    "        self.n_X = 0\n",
    "        self.n_Y = 0\n",
    "        self.R = None\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.max_val = None\n",
    "        self.min_val = None\n",
    "        self.train_mask = None\n",
    "        self.val_mask = None\n",
    "        self.current_X_tr_ind = 0\n",
    "        self.current_Y_tr_ind = 0\n",
    "        self.current_X_val_ind = 0\n",
    "        self.current_Y_val_ind = 0\n",
    "\n",
    "    def load_data(self):\n",
    "        self.R = scipy.sparse.load_npz(self.rating_fname)\n",
    "        self.N, self.M = self.R.shape\n",
    "        print('Original data: %d x %d' %(self.R.shape[0], self.R.shape[1]))\n",
    "        val_set = np.unique(self.R.data)\n",
    "        self.min_val = float(val_set[0]) \n",
    "        self.max_val = float(val_set[-1])\n",
    "        self.train_mask = scipy.sparse.load_npz(self.tr_m_fname).astype(np.float32)\n",
    "        self.val_mask = scipy.sparse.load_npz(self.v_m_fname).astype(np.float32)\n",
    "        print('Finished loading data')\n",
    "        self.X_tr_indices = np.arange(self.N)\n",
    "        self.Y_tr_indices = np.arange(self.M)\n",
    "        self.X_val_indices = np.arange(self.N)\n",
    "        self.Y_val_indices = np.arange(self.M)\n",
    "        print('Finished initializing indices')\n",
    "\n",
    "    def split(self):\n",
    "        self.R_tr_unnormalized = self.R.multiply(self.train_mask)  \n",
    "        self.R_val_unnormalized = self.R.multiply(self.val_mask)\n",
    "        self.X_tr = self.R_tr_unnormalized.copy()\n",
    "        self.Y_tr = self.R_tr_unnormalized.copy().T.tocsr()\n",
    "\n",
    "    def shuffle_indices(self, for_x=False, for_y=False):\n",
    "        if for_x:\n",
    "            print('Shuffle train X indices')\n",
    "            self.X_tr_indices = np.random.permutation(range(self.N))\n",
    "        if for_y:\n",
    "            print('Shuffle train Y indices')\n",
    "            self.Y_tr_indices = np.random.permutation(range(self.M))\n",
    "\n",
    "    def get_num_samples(self, dataset):\n",
    "        if dataset == 'train':\n",
    "            return self.x_train.shape[0]\n",
    "        if dataset == 'val':\n",
    "            return self.x_val.shape[0]\n",
    "\n",
    "    def get_X_dim(self):\n",
    "        return self.X_tr.shape[1]\n",
    "\n",
    "    def get_Y_dim(self):\n",
    "        return self.Y_tr.shape[1]\n",
    "\n",
    "    def next_full_batch(self, dataset):\n",
    "        X_set = None\n",
    "        Y_set = None\n",
    "        mask = None\n",
    "        X_set = self.X_tr\n",
    "        Y_set = self.Y_tr\n",
    "        if dataset == 'train':\n",
    "            mask = self.train_mask\n",
    "        elif dataset == 'val':\n",
    "            mask = self.val_mask\n",
    "        R = X_set\n",
    "        return X_set, Y_set, R, mask.astype(np.float32), True\n",
    "\n",
    "    def get_toread_indices(self, current, all_indices, batch_size):\n",
    "        ''' Get indices of samples to-be-read from all_indices\n",
    "        '''\n",
    "        start = current\n",
    "        end = current + batch_size\n",
    "        n_samples = all_indices.shape[0]\n",
    "        to_read = 0\n",
    "        flag = False\n",
    "        if end > n_samples:\n",
    "            to_read = end - n_samples\n",
    "            end = n_samples\n",
    "            flag = True\n",
    "        to_read_indices = all_indices[start:end]\n",
    "        start = end   \n",
    "        if to_read > 0:\n",
    "            to_read_indices = np.append(to_read_indices, all_indices[0:to_read])     \n",
    "            start = 0\n",
    "        return to_read_indices, start, flag\n",
    "\n",
    "    def get_elements(self, R, x_indices, y_indices):\n",
    "        ''' get values of all pairs of selected rows and columns '''\n",
    "        n = x_indices.shape[0]\n",
    "        m = y_indices.shape[0]\n",
    "        values = np.zeros((n,m))\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                values[i,j] = R[x_indices[i],y_indices[j]]\n",
    "        return values\n",
    "\n",
    "    def get_elements_vectorized(self, R, x_indices, y_indices):\n",
    "        ''' vectorized implementation of get_ratings functions. Gain 3x speedup '''\n",
    "        n = x_indices.shape[0]\n",
    "        m = y_indices.shape[0]\n",
    "        values = np.zeros((n,m)).astype(np.float32)\n",
    "        value_ind1, value_ind2 = np.meshgrid(x_indices, y_indices)\n",
    "        ind1, ind2 = np.meshgrid(range(n), range(m))\n",
    "        values[ind1.flatten(),ind2.flatten()] = R[value_ind1.flatten(), value_ind2.flatten()]\n",
    "        return values\n",
    "\n",
    "    def next_batch(self, bs_x, bs_y, dataset, verbose=False):\n",
    "        ''' read next batch of input\n",
    "        '''\n",
    "        if dataset == 'train':\n",
    "            start_x = self.current_X_tr_ind\n",
    "            start_y = self.current_Y_tr_ind\n",
    "            all_x_indices = self.X_tr_indices\n",
    "            all_y_indices = self.Y_tr_indices\n",
    "            full_mask = self.train_mask\n",
    "            full_R = self.R_tr_unnormalized\n",
    "        elif dataset == 'val':\n",
    "            start_x = self.current_X_val_ind\n",
    "            start_y = self.current_Y_val_ind\n",
    "            all_x_indices = self.X_val_indices\n",
    "            all_y_indices = self.Y_val_indices\n",
    "            full_mask = self.val_mask\n",
    "            full_R = self.R_val_unnormalized\n",
    "        else:\n",
    "            assert False, 'Invalid dataset'\n",
    "        x_indices, start_x, flag_x = self.get_toread_indices(start_x, all_x_indices, bs_x)\n",
    "        y_indices, start_y, flag_y = self.get_toread_indices(start_y, all_y_indices, bs_y)\n",
    "\n",
    "        start = time.time()\n",
    "        x = self.X_tr[x_indices,:].todense()\n",
    "        y = self.Y_tr[y_indices,:].todense()\n",
    "        if verbose:\n",
    "            print('Load dense x and y takes %f s' %(time.time() - start))\n",
    "\n",
    "        R = self.get_elements_vectorized(full_R, x_indices, y_indices)\n",
    "        mask = self.get_elements_vectorized(full_mask, x_indices, y_indices)\n",
    "        \n",
    "        start = time.time()\n",
    "        # scale R to be in range [-1,1]\n",
    "        mid = (self.max_val + self.min_val) / 2\n",
    "        R = (R - mid) / (mid - self.min_val)\n",
    "\n",
    "        if dataset == 'train':\n",
    "            self.current_X_tr_ind = start_x\n",
    "            self.current_Y_tr_ind = start_y \n",
    "        elif dataset == 'val':\n",
    "            self.current_X_val_ind = start_x\n",
    "            self.current_Y_val_ind = start_y\n",
    "\n",
    "        if dataset == 'train':\n",
    "            if flag_x:\n",
    "                self.shuffle_indices(for_x=True)\n",
    "            if flag_y:\n",
    "                self.shuffle_indices(for_y=True)\n",
    "        flag = flag_x or flag_y\n",
    "        return x, y, R, mask, flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ModelConfig():\n",
    "    u_hidden_sizes = [2048,1024]    # hidden layers's size for the row branch\n",
    "    v_hidden_sizes = [2048,1024]    # hidden layers's size for the column branch\n",
    "    dropout_keep_prob = 0.1         # dropout rate = 1.0 - dropout_keep_prob\n",
    "    use_bn = True                   # use batch normalization after fully-connected layer\n",
    "    activation_fn = 'relu'          # activation function\n",
    "    summarization = False           # user summarization layers \n",
    "    n_u_summ_filters = [32]         # no. conv filters in summarization layers in the row branch\n",
    "    n_v_summ_filters = [32]         # no. conv filters in summarization layers in the column branch\n",
    "    u_summ_layer_sizes = [20]       # conv filter sizes in summarization layers in the row branch\n",
    "    v_summ_layer_sizes = [10]       # conv filter sizes in summarization layers in the column branch\n",
    "\n",
    "class TrainConfig(object):\n",
    "    \"\"\"Sets the default training hyperparameters.\"\"\"\n",
    "    batch_size_x = 100\n",
    "    batch_size_y = 170              # should be set accordingly to the ratio between row and column of the original matrix \n",
    "\n",
    "    initial_lr = 1e-2               # initial learning rate\n",
    "    lr_decay_factor = 0.65          # learning rate decay factor\n",
    "    num_epochs_per_decay = 50       # decay learning every ? epochs\n",
    "    n_epochs = 1000                 # number of training epochs (1 epoch is one round passing through all the rows or columns)\n",
    "    save_every_n_epochs = 500       # saving model every ? epochs\n",
    "    log_every_n_steps = 20          # print training log every ? steps\n",
    "\n",
    "    weight_decay = 0.0              # weight of the l2 regularization \n",
    "\n",
    "def arr_to_string(arr):\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = str(arr[i])\n",
    "    return ','.join(arr)\n",
    "\n",
    "# model configs\n",
    "tf.flags.DEFINE_string('u_hidden_sizes', arr_to_string(ModelConfig.u_hidden_sizes),'')\n",
    "tf.flags.DEFINE_string('v_hidden_sizes', arr_to_string(ModelConfig.v_hidden_sizes),'')\n",
    "tf.flags.DEFINE_float('dropout_keep_prob', ModelConfig.dropout_keep_prob,'')\n",
    "tf.flags.DEFINE_boolean('use_bn', ModelConfig.use_bn,'')\n",
    "tf.flags.DEFINE_string('activation_fn', ModelConfig.activation_fn,'')\n",
    "tf.flags.DEFINE_boolean('summarization', ModelConfig.summarization,'')\n",
    "tf.flags.DEFINE_string('n_u_summ_filters', arr_to_string(ModelConfig.n_u_summ_filters),'')\n",
    "tf.flags.DEFINE_string('n_v_summ_filters', arr_to_string(ModelConfig.n_v_summ_filters),'')\n",
    "tf.flags.DEFINE_string('u_summ_layer_sizes', arr_to_string(ModelConfig.u_summ_layer_sizes),'')\n",
    "tf.flags.DEFINE_string('v_summ_layer_sizes', arr_to_string(ModelConfig.v_summ_layer_sizes),'')\n",
    "\n",
    "# training configs\n",
    "tf.flags.DEFINE_integer('batch_size_x', TrainConfig.batch_size_x,'')\n",
    "tf.flags.DEFINE_integer('batch_size_y', TrainConfig.batch_size_y,'')\n",
    "tf.flags.DEFINE_float('initial_lr', TrainConfig.initial_lr,'')\n",
    "tf.flags.DEFINE_float('lr_decay_factor', TrainConfig.lr_decay_factor,'')\n",
    "tf.flags.DEFINE_integer('num_epochs_per_decay', TrainConfig.num_epochs_per_decay,'')\n",
    "tf.flags.DEFINE_integer('n_epochs', TrainConfig.n_epochs,'')\n",
    "tf.flags.DEFINE_integer('save_every_n_epochs', TrainConfig.save_every_n_epochs,'')\n",
    "tf.flags.DEFINE_integer('log_every_n_steps', TrainConfig.log_every_n_steps,'')\n",
    "tf.flags.DEFINE_float('weight_decay', TrainConfig.weight_decay,'')\n",
    "\n",
    "CONFIGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "# import configs.configs_ML100K as configs\n",
    "# from model import NMC\n",
    "# from data_loader import DataLoader\n",
    "import time \n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.flags.DEFINE_string(\"data_dir\", \"./data/MovieLens100K/\", \"Data directory.\")\n",
    "tf.flags.DEFINE_string(\"output_basedir\", \"./outputs/\", \"Directory for saving and loading model checkpoints.\")\n",
    "tf.flags.DEFINE_string(\"pretrained_fname\", \"\", \"Name of the pretrained model checkpoints (to resume from)\")\n",
    "tf.flags.DEFINE_string(\"output_dir\", \"\", \"Model output directory.\")\n",
    "FLAGS.output_dir = FLAGS.output_basedir + 'snapshots/snapshot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(FLAGS.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: 943 x 1682\n",
      "Finished loading data\n",
      "Finished initializing indices\n"
     ]
    }
   ],
   "source": [
    "dl.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dl.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_dim = dl.get_X_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_dim = dl.get_Y_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfgs = CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating training directory: ./outputs/snapshots/snapshot\n"
     ]
    }
   ],
   "source": [
    "assert FLAGS.output_dir, \"--output_dir is required\"\n",
    "    # Create training directory.\n",
    "output_dir = FLAGS.output_dir\n",
    "if not tf.gfile.IsDirectory(output_dir):\n",
    "    tf.logging.info(\"Creating training directory: %s\", output_dir)\n",
    "    tf.gfile.MakeDirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, R, mask, flag = dl.next_batch(cfgs.batch_size_x, cfgs.batch_size_y, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1682)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 943)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 170)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 170)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 5.,  3.,  4.,  3.,  0.,  5.,  4.,  1.,  5.,  0.,  0.,  0.,  5.,\n",
       "          5.,  0.,  5.,  3.,  4.,  0.,  0.,  1.,  0.,  4.,  3.,  4.,  0.,\n",
       "          2.,  4.,  1.,  3.,  3.,  0.,  4.,  2.,  0.,  2.,  2.,  3.,  4.,\n",
       "          3.,  2.,  5.,  0.,  0.,  0.,  0.,  4.,  5.,  3.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 5.,  4.,  0.,  0.,  4.,  4.,  0.,  0.,  0.,  4.,  0.,  0.,  3.,\n",
       "          0.,  1.,  0.,  4.,  0.,  0.,  3.,  5.,  0.,  0.,  0.,  5.,  3.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,\n",
       "          0.,  0.,  5.,  5.,  4.,  5.,  0.,  0.,  0.,  2.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0,0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1. ,  0. ,  0.5,  0. , -1.5,  1. ,  0.5, -1. ,  1. , -1.5, -1.5,\n",
       "       -1.5,  1. ,  1. , -1.5,  1. ,  0. ,  0.5, -1.5, -1.5, -1. , -1.5,\n",
       "        0.5,  0. ,  0.5, -1.5, -0.5,  0.5, -1. ,  0. ,  0. , -1.5,  0.5,\n",
       "       -0.5, -1.5, -0.5, -0.5,  0. ,  0.5,  0. , -0.5,  1. , -1.5, -1.5,\n",
       "       -1.5, -1.5,  0.5,  1. ,  0. , -1.5], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R[0,0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,\n",
       "        1.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,\n",
       "        1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0,0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
